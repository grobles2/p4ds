{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Binary Classification\n",
    "\n",
    "This notebook is heavily inspired by Andre Guernon work, that can be found here: https://github.com/ageron/handson-ml/blob/master/04_training_linear_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥ 3.9 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 9)\n",
    "\n",
    "# Scikit-Learn ≥1.0 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "We will use `sklearn.datasets.fetch_openml()` to fetch dataset from openml by name or dataset id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\n",
    "    'mnist_784',\n",
    "    version=1,\n",
    "    as_frame=False # we want the dataset as NumPy ndarray not as a pandas DataFrame\n",
    ")\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist['DESCR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset, inputs and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` contains 70,000 images each of them contains 784 features, because each of them is a 28x28 picture. Each feature is a pixel intensity encoded in an 8-bit scale: from 0 (white) to 255 (black)\n",
    "\n",
    "Let's display one or more images using matplotlib `imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = X[9]\n",
    "digit_img = digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(digit_img, cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is a string. We must convert it to a number for it to work on a Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function to show a bunch of digit images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digits(instances, images_per_row=10, **opts):\n",
    "    \"\"\"\n",
    "    Utility function to display the MNIST digits on a grid\n",
    "    \"\"\"\n",
    "    # the size of our images (28x28)\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # convert images from 1-D to 2-D arrays\n",
    "    images = [instance.reshape(size, size) for instance in instances]\n",
    "    # compute how many rows you need in the grid\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    # create empty \"dummy\" images to fill potential remaining slots at the end of the grid\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    # concatenate all the images in a single grid image\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    # plot the grid image\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **opts)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "show_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in training and test set\n",
    "\n",
    "Well set aside 10,000 samples for testing purposes. The data set is already shuffled for us so we can just take the last 10,000 samples for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use slicing to create training and test set\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating a binary classifier\n",
    "\n",
    "Let's define two set of labels for the training and test set, named `y_train_8` and `y_test_8`. These must contain the value `True` whenever the original label is an 8, `False` otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented using boolean masking\n",
    "y_train_8 = y_train == 8\n",
    "y_test_8 = y_test == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_8[:20], y_train[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**: \n",
    "Let's suppose we want to implement a binary classfier to classify even vs odd digits. Define two set of labels for the training and test set, named `y_train_even` and `y_test_even`. These must contain the value `True` whenever the original label is a digit representing an even number, `False` if it's an odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression classifier\n",
    "\n",
    "We can train a logistic regression classifier by either using `sklearn.linear_model.LogisticRegression` or by using `sklearn.linear_model.SGDClassifier` with `loss` argument set as `log` (or `log_loss` if using `v1.1+`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_cl = LogisticRegression()\n",
    "log_cl.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question:** \n",
    "A warning crops up if running the cell above. What is the issue? How could we fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempt a solution here (at home)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some predictions\n",
    "log_cl.predict(X_train[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise:**\n",
    "Let's try using an `SGDClassifier` to fit a logistic regressor using Stochastic Gradient Descent rather than the explicit solution.\n",
    "\n",
    "**NB:** Check the SGDClassifier [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) to see what parameter you need to pass to `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "\n",
    "sgd_cl = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cl.predict(X_train[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has correctly predicted the \"8\" at index 17. However this belongs to the data it used during the training phase.\n",
    "We need a validation set to fairly evaluate the performance of our logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Performance Measures: measuring Accuracy Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use `cross_val_score()` to assess the accuracy of our Classifier `sgd_cl` on (X_train, y_train), using 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_cl, X_train, y_train_8, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is 93 % in the first two runs and 84% in the third. The classifier looks very performant, but is it really the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise (at home):** \n",
    "Use `cross_val_score()` to assess the accuracy of an SGD classifier implementing an online support vector machine (SVM), on (X_train, y_train), using 5-fold cross-validation.\n",
    "\n",
    "It is more or less accurate than the SGD classifier implementing logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dummy classifier that never predicts that a digit is an \"8\". It will just always return `False` (i.e. `0`) as a predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never8Classifier(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool) \n",
    "never_8_clf = Never8Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3:** \n",
    "Use `cross_val_score()` to assess the accuracy of our Classifier `never_8_clf` on (X_train, y_train), using 3-fold cross-validation. Which accuracy do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has over 90% accuracy! This is simply because only about 10% of the images are 8s, so if you always guess that an image is not a 8, you will be right about 90% of the time. \n",
    "\n",
    "Accuracy _per_ _se_ is not the preferred metrics when dealing with classifiers. This is even more true in this case, as we are dealing with a skewed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more reliable way to measure the performance of a classifier is to look at the so-called _confusion matrix_. The aim is to quantify how many times members of a class C1 are misclassified as members of the class C2. To do that we will use the `cross_value_predict()` (rather the CV scores, it returns the predictions) function together with the `confusion_matrix()` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(\n",
    "    sgd_cl, X_train, y_train_8, cv=3\n",
    ")\n",
    "confusion_matrix(y_train_8, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal perfect classifier would have true positives and true negatives only. In this case the confusion matrix would have zero values outside the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_pred = y_train_8\n",
    "confusion_matrix(\n",
    "    y_train_8, \n",
    "    y_train_perfect_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Precision, Recall and Harmonic mean (F1 score)\n",
    "\n",
    "* Precision or Positive Predicted Value: $$ PPV = \\frac{TP}{TP + FP} $$\n",
    "* Recall or TPR or Sensitivity $$ TPR  = \\frac{TP}{TP + FN} $$\n",
    "* Harmonic Mean of Precision and Recall $$ F1 = \\frac{2}{\\frac{1}{Precision}+\\frac{1}{Recall}} = \\frac{TP}{TP + \\frac{FN+FP}{2}} $$\n",
    "\n",
    "### **Exercise:** \n",
    "compute precision, recall and F1-score for our `sgd_cl` we have trained above with `cross_val_predict()` using the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our classifier should look not as good than before. It has a 50% precision and 68% recall. \n",
    "\n",
    "Notice that F1 tends to favour models that have similar precision and recall. But in some context you might prefer a higher precision, while in others a higher recall, depending on the task.\n",
    "\n",
    "There is however a trade-off between precision and recall.\n",
    "\n",
    "### Precision/Recall trade-off\n",
    "\n",
    "A classifier such as our `SGDClassifier` performs the classification task by computing a score based on a \"decision function\". If a score is greater than a given threshold value, the instance is labeled with the positive class, otherwise with the negative class (from the theory of Logistic regression, if you remember, an estimated probabilty of class \"1\" greater than 0.5 means that we assign the value to class \"1\"). Raising this threshold will reduce the number of FP, thus increasing the precision. However, it will also increase the number of FN thus reducing the recall score.\n",
    "\n",
    "Let's try to manipulate the `SGDClassifier`'s threshold manually, using the classifier's `.decision_function()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_cl.decision_function(X_train[:5])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_pred_on_scores = y_scores > threshold\n",
    "y_pred_on_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2000\n",
    "y_pred_on_scores = y_scores > threshold\n",
    "y_pred_on_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising the threshold increases the number of FN, decreasing the recall. How can we than determine the right threshold value for our task? First let's use `cross_val_predict()`using the 'decision_function' method on our entire training set, and then let's use the computed score together with the `precision_recall_curve()` to compute precision and recall for all the possible threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "y_scores = cross_val_predict(\n",
    "    sgd_cl,\n",
    "    X_train,\n",
    "    y_train_8,\n",
    "    cv=3,\n",
    "    method='decision_function'\n",
    ")\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_train_8, y_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_precision_and_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    sns.lineplot(x=thresholds, y=precisions[:-1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.lineplot(x=thresholds, y=precisions[:-1], ax=ax)\n",
    "sns.lineplot(x=thresholds, y=recalls[:-1], ax=ax)\n",
    "plt.legend(['Precision', 'Recall'], loc=\"center right\", fontsize=16) \n",
    "plt.xlabel(\"Threshold\", fontsize=16)        \n",
    "plt.grid(True)                                      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the threshold for which we can achieve a 90% precision\n",
    "threshold_90_prec = thresholds[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= threshold_90_prec)\n",
    "precision_score(y_train_8, y_train_pred_90), recall_score(y_train_8, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have reached a 90% precision, at the expense of recall, which is now 18%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: The ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tool that can be used to evaluate a classifier performance is the receiver-operating curve. The ROC curves plots the true positive rate (TPR, i.e. recall) vs the false positive rate (FPR). \n",
    "\n",
    "Specificity or TNR: $$ TNR  = \\frac{TN}{TN + FP} $$\n",
    "\n",
    "\n",
    "False positive rate: $$FPR = 1 - TNR$$\n",
    "The ROC curve plots sensitivity agains (1-specificity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_8, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') \n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n",
    "    plt.grid(True)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can measure the _area under the curve_ (AUC) if you want to compare the performance of different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_8, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, prefer the Precision/Recall curve if the positive class is uncommon or if you worry more about the false positives rather than the false negatives. In the other scenarios, prefer the ROC curve.\n",
    "\n",
    "#### ROC AUC for KNNs and Random Forests\n",
    "\n",
    "Let's try two different classifiers: a K-Nearest Neighbours classifier and a Random Forest classifier.\n",
    "\n",
    "The K-Nearest Neighbours algorithm checks the K closest (i.e. most similar instances) in the training set and assigns as predicted class for the new instance the most represented class in the neighbourhood. \n",
    "\n",
    "The Random Forest algorithm is an ensemble method which trains a number of decision tree classifiers on various sub-samples of the training set and uses averaging techniques to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "We will see more on Decision Trees and Ensemble methods next week.\n",
    "\n",
    "**NOTE:** K-Nearest Neighbour and Random Forest classifiers do not have a `decision_function()` method that returns the predicted scores for each instance in cross validation. They do have, however, a `predict_proba()` method that returns an array containing a row per instance and a column per class. This array contains the predicted probability that each instance belongs to a class. This can be used to draw ROC curves in lieu of `decision_function()`. Scikit-learn classifiers usually implement either one or the other method so you need to check their API to find out the one you need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kn_cl = KNeighborsClassifier(n_neighbors=9)\n",
    "y_probs_kn = cross_val_predict(\n",
    "    kn_cl,\n",
    "    X_train,\n",
    "    y_train_8,\n",
    "    cv=3, \n",
    "    method='predict_proba'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_cl = RandomForestClassifier(\n",
    "    n_estimators=100, # a \"forest\" of 100 decision trees\n",
    "    random_state=77\n",
    ")\n",
    "y_probs_forest = cross_val_predict(\n",
    "    forest_cl,\n",
    "    X_train,\n",
    "    y_train_8,\n",
    "    cv=3,\n",
    "    method=\"predict_proba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check `y_probs_forest` you will see that it has a column with the probabilities for the `0` class and a column for the probabilities for the `1` class, one row per sample. Each row adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbours scores\n",
    "y_scores_kn = y_probs_kn[:, 1]    # score = proba of positive class\n",
    "fpr_kn, tpr_kn, thresholds_kn = roc_curve(y_train_8, y_scores_kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise:** \n",
    "let's compute the scores, TPR, FPR for the Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "y_scores_forest = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise:** \n",
    "Use the scores computed above for `sgd_cl`, `kn_cl`, `forest_cl` to plot out the three ROC curves on the same plot. You can use, if you wish the `plot_roc_curve()` function defined above. Afterward compute the area under the curve for `mnb_cl` and `forest_cl`. Which is the best and the worst classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here:\n",
    "plt.figure(figsize=(14, 8))\n",
    "...\n",
    "...\n",
    "...\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_8, y_scores_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_8, y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise (at home)**: \n",
    "train and evaluate a [Logistic Regressor classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) and a [Linear SVM classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) in 5-fold cross-validation using the even vs odd labels we have defined above (`y_train_even`, `y_test_even`). Try and choose e sound combination of hyperparameters. What metric could you use for evaluation? Would accuracy be a good choice?\n",
    "\n",
    "Plot the ROC curves for the two classifiers and compute the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution down here\n",
    "# Add as many cells as you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum: more flexible cross-validation\n",
    "\n",
    "Sometimes you need more control over cross-validation than what is offered out of the box with `cross_val_score()`. In the example in the cell below we are going to use the `StratifiedKFold` class to implement cross-validation\n",
    "\n",
    "#### <span style=\"color:red\">**NB: this cell will take quite a while to run. Be patient**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=77, shuffle=True)\n",
    "\n",
    "for train_index, val_index in skfolds.split(X_train, y_train_8):\n",
    "    # make a clone (copy) of our Stochastic Gradient Classifier\n",
    "    clone_sgd_cl = clone(sgd_cl) \n",
    "    \n",
    "    # get training and validation set for current CV iteration\n",
    "    X_train_f = X_train[train_index]\n",
    "    X_val_f = X_train[val_index]\n",
    "    y_train_f = y_train[train_index]\n",
    "    y_val_f = y_train[val_index]\n",
    "    \n",
    "    # train the SGD classifier\n",
    "    clone_sgd_cl.fit(X_train_f, y_train_f)\n",
    "    # make predictions on validation set\n",
    "    y_pred = clone_sgd_cl.predict(X_val_f)\n",
    "    # count number of correct predictions\n",
    "    n_correct = sum(y_pred == y_val_f)\n",
    "    # print out accuracy score\n",
    "    print(n_correct / len(y_val_f))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
