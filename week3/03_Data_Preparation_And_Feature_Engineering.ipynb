{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abff593-e7be-4a52-91ff-c125c746fb72",
   "metadata": {},
   "source": [
    "# Week 3-I: Data Preparation on the Kings County Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918442ff-dc9b-4514-8d27-edf2f981604d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b21c6-1702-4e43-b91a-ac6e9e32e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.9 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 9)\n",
    "\n",
    "# Scikit-Learn ≥1.0 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Precision options\n",
    "np.set_printoptions(precision=2)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858dc77-89f7-4e41-b945-062169dc7c58",
   "metadata": {},
   "source": [
    "## 1. Get the Data + Train/test split (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439c8f0-96eb-4894-9b5f-e50f8efc01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\n",
    "    \"../datasets/kings_county_house_data.csv\",\n",
    "    dtype={'zipcode': str}   # US ZIP codes look like numbers but we want to treat them like strings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbb592-7c1e-4d48-8750-e8fb97c1dc49",
   "metadata": {},
   "source": [
    "Perform a stratified split wrt `sqft_living`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa2282-bad6-42a5-bd5d-2be254b09545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing[\"sqft_living_cat\"] = pd.cut(\n",
    "    housing.sqft_living, \n",
    "    bins=[0., 1000., 2000., 3000., 4000., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in splitter.split(housing, housing.sqft_living_cat):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]\n",
    "# delete the \"sqft_living_cat\"columns  \n",
    "for set_ in (train_set, test_set):\n",
    "    set_.drop(\"sqft_living_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad770e05-1d12-46b6-bb94-27129997aa41",
   "metadata": {},
   "source": [
    "## 2. Data Preparation Plan\n",
    "\n",
    "From the EDA we have defined the following data preparation plan:\n",
    "    \n",
    "* We have identified a cutoff at latitude  ~47.5 between more expensive houses and cheaper houses.We will create a binary engineered feature to capture this. We could also create a cutoff at -126.1 long to separate the urban west from the rural east of the county. We will then remove `lat` and `long`.\n",
    "* We have decided to discard `sqft_living15` and `sqft_living_above` in favour of `sqft_living`\n",
    "* We have decided to add a binary engineered feature that indicates whether a house has a basement or not. We will then remove the continuous variable `sqft_basement`\n",
    "* We will create a renovated binary flag. If a house is older than 25 years (relative to the most recent data in the dataset) and has not been renovated we will set renovated to 0, otherwise to 1. We will then remove the continuous variable `yr_built` and `yr__renovated`\n",
    "* We have decided to collapse the 70 zipcodes into 9 zipcode groups based on average house prices in the zipcodes. This will be performed by `make_zipcode_groups()` function and you do not have to worry about the implementation details of it.\n",
    "* Some houses report 0 bathrooms. We need to replace those values with more meaningful estimates.\n",
    "* One house has 33 bedrooms. We will replace that value with 3, as it looks like a reporting mistake.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56747202-a063-40e0-8baa-f96dc8768e35",
   "metadata": {},
   "source": [
    "## 3 Data preparation: check there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a4fce-e64d-409d-bcd0-b3ab31fdf6b4",
   "metadata": {},
   "source": [
    "We can use pd.DataFrame.isna() or pd.DataFrame.isnull() to look for null or missing values in any of our variables/features.\n",
    "\n",
    "NOTE: axis=1 performs the operation along the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61d46a-a53d-4985-881e-4d16c9e8319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look for rows with incomplete values\n",
    "incomplete_rows = train_set[train_set.isna().any(axis=1)]\n",
    "incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1fc18-aebe-40a4-b312-87a84816b7ad",
   "metadata": {},
   "source": [
    "There are no missing values in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee4cec-0a54-479c-9bdb-843afd0a3af8",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### **Exercise:** fill the missing feature engineering steps in the `engineer_features()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9520f-5aa4-4490-be42-d466975a143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kings_county_utils import make_zipcode_groups, assign_to_zipcode_group\n",
    "\n",
    "def engineer_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # let's make a copy of the original dataset\n",
    "    engineered_data = data.copy()    \n",
    "    # create a variable \"north_loc\", 1 for latitudes greather than 47.5\n",
    "    engineered_data[\"north_loc\"] = np.where(engineered_data[\"lat\"] > 47.5, 1, 0)\n",
    "    # create a variable \"rural_east\", 1 for longitudes greater than -121.6\n",
    "    ...\n",
    "    # drop \"lat\" and \"long\"\n",
    "    engineered_data = engineered_data.drop(columns=[\"lat\", \"long\"])\n",
    "    # drop \"sqft_living15\" and \"sqft_living_above\"\n",
    "    engineered_data = engineered_data.drop(columns=[\"sqft_living15\", \"sqft_above\"])\n",
    "    # add a binary variable for the presence of a  basement, 1 if true 0 otherwise\n",
    "    ...\n",
    "    # drop \"sqft_basement\"\n",
    "    engineered_data = engineered_data.drop(columns=[\"sqft_basement\"])\n",
    "    # create an \"is_renovated\" binary variable\n",
    "    max_year = max(engineered_data[\"yr_built\"].max(), engineered_data[\"yr_renovated\"].max())\n",
    "    engineered_data[\"renovated\"] = np.where(\n",
    "        (engineered_data[\"yr_built\"] > max_year - 25) | (engineered_data[\"yr_renovated\"] > 0), 1, 0\n",
    "    )\n",
    "    # drop \"yr_built\" and \"yr_renovated\"\n",
    "    engineered_data = engineered_data.drop(columns=[\"yr_built\", \"yr_renovated\"])\n",
    "    # group zipcodes into groups (as we did last week)\n",
    "    zipcode_groups = make_zipcode_groups(engineered_data)\n",
    "    engineered_data[\"zipcode_group\"] = engineered_data[\"zipcode\"].apply(assign_to_zipcode_group, zipcode_groups=zipcode_groups)\n",
    "    # drop \"zipcode\"\n",
    "    engineered_data = engineered_data.drop(columns=[\"zipcode\"])\n",
    "    # replace 0 bathrooms with NaN (we will fill it later on)\n",
    "    ...\n",
    "    # replace 33 bedrooms with Nan (we will fill it later on)\n",
    "    engineered_data.loc[engineered_data['bedrooms'] == 33, 'bedrooms'] = np.nan\n",
    "    # drop id, date and price (as we won't use them)\n",
    "    ...\n",
    "    return engineered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a9b46-2d0d-4fe1-9a73-4085cadd0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = engineer_features(train_set)\n",
    "train_labels = train_set[\"price\"].copy()\n",
    "train_data.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f4c22-4e26-4681-8d84-60f3a83ddd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4835e-a6ee-4381-91d4-b76fc69098d6",
   "metadata": {},
   "source": [
    "We now have 15 features in our dataset, but as we will see when preparing our data for feeding it into regression models we will get a few extra \"dummy\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c5627-7dc5-43bc-85ca-8589757190a6",
   "metadata": {},
   "source": [
    "## 3 Check there are no missing values in the engineered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece59e6f-9979-4f66-9487-5e9113ad547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look for rows with incomplete values\n",
    "incomplete_rows = train_data[train_data.isna().any(axis=1)]\n",
    "incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a97ab-cd89-4991-823e-5d1fcd25757e",
   "metadata": {},
   "source": [
    "As we can see we now have 1 sample with missing bedrooms info and 7 samples with missing bathrooms info. We need to deal with this missing values. The problem of dealing with missing values is called imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f147b6-2d18-4ea6-b9cb-f60246028319",
   "metadata": {},
   "source": [
    "### 3.1 Imputation\n",
    "\n",
    "In statistics, imputation is the process of replacing missing data with substituted values.\n",
    "In scikit-learn we can use the `SimpleImputer` calss to perform univariate imputation on missing  values. Generally we wil want to replace missing numeric (quantitative) and ordinal values with the median value of that feature. For categorical features we may want to either use a \"missing\"/\"unknown\" category, use the mode, or drop the samples with missing values.\n",
    "\n",
    "Let's fill in the missing ordinal values of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91183a-93a6-43fe-bd58-5bc95cfe7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "train_data_num = train_data.select_dtypes(include=[np.number])\n",
    "train_data_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dff039-193b-4bff-b692-984410dfee69",
   "metadata": {},
   "source": [
    "Basically of our fields, with the exception of `zipocode_group` is quantitative or ordinal (or a binary variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009de44-dcf6-423f-81b1-4710f02ccbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(train_data_num)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65b5cb-4770-4c27-aa81-27e0d9d661cb",
   "metadata": {},
   "source": [
    "The `SimpleImputer.statistics_` property is just the median value for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799a2b7-d719-49e9-ba43-3e34d1206dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7ae83-bc55-4118-a72c-663b87ed133b",
   "metadata": {},
   "source": [
    "We can now fill the missing values by applying the `transform()` method of the imputer to `train_data_num`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc678d1-cfc8-4595-9ec5-ae5cb78a846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_arr = imputer.transform(train_data_num)\n",
    "# imputer.transform return a NumPy array, we need to wrapp it back into a dataframe with column names and index\n",
    "train_data_num_filled = pd.DataFrame(\n",
    "    train_data_arr,\n",
    "    columns=train_data_num.columns,\n",
    "    index=train_data_num.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3f974-e8ae-4d3a-80b2-ce5ac74e78b1",
   "metadata": {},
   "source": [
    "We can now verify that there are no more rows with missing values in `train_data_num`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2087954-dacf-4e8a-b2c8-67e70b681c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num_filled[train_data_num_filled.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02663b3a-bd01-4f17-8659-00886bfe1838",
   "metadata": {},
   "source": [
    "### 3.2 Handling Categorical Attributes\n",
    "\n",
    "In general we can consider three types of text features: categorical, ordinal, and unstructured.\n",
    "Unstructured text is more the subject of Natural Language Processing, hence we will not consider its processing/encoding at this stage (and we have no unstructured data). Ordinal data are text categories that imply and intrinsic order such as the set (\"BAD\", \"AVERAGE\", \"GOOD\", \"VERY GOOD\", \"EXCELLENT\"). \n",
    "These are generally encodes as integers (\"BAD\" => 0, \"AVERAGE\" => 1, \"GOOD\" => 2, \"VERY GOOD\" => 3, \"EXCELLENT\" => 4). These transformations can be handed with custom functions as above or using `sklearn.preprocessing.OrdinalEncoder`. In our case all the ordinal features are already expressed as numbers so we don't need to do anything with them.\n",
    "\n",
    "To handle Categorical Attributes that are not ordinal, a common solution is to create one binary attribute per category. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called *dummy attributes*. Scikit-Learn provides a `sklearn.preprocessing.OneHotEncoder` class to convert categorical values into one-hot vectors.\n",
    "\n",
    "In our case we have the \"zipcode\" attribute that can be considered as categorical. Each \"zipcode\" category should become a mutually exclusive dummy attribute\n",
    "\n",
    "### <b>Exercise:</b> Use the `OneHotEncoder` class to encode each ZIP code as a separate category. Check the documentation for appropriate use of the `OneHotEncoder` transformer. What kind of output do you get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b76f56-adbb-45db-ae74-c89f5ffa2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887522-4924-424e-9a70-99dfc0acff82",
   "metadata": {},
   "source": [
    "Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. We can get a dense array out of sparse matrix by calling the `.toarray()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0d568-6e1c-435b-9fc4-7e0fab3ac2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade7be1-2d1b-4b73-bf86-0f767228fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cat_1hot.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4e6af-d930-4cbc-858c-d6f4876492aa",
   "metadata": {},
   "source": [
    "We can get the \"column\" names by checking the `OneHotEncoder.categories_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc367c-fd16-44b6-a291-7fb770e6f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33838dfa-ec0c-49e9-aedd-bf1d62fe9411",
   "metadata": {},
   "source": [
    "### 3.3 Custom Transformers\n",
    "\n",
    "You can define you own transformers creating a class that inherits both from `BaseEstimator` (so that it inherits the `.fit()` method) and the mixin class `TransformerMixin` (so that it acquires the `.tranform()` method)\n",
    "\n",
    "\n",
    "### **Exercise:** convert `engineering_features()` to a scikit-learn's transformer subclassing `BaseEstimator` and `TransformerMixin`. I am already providing you with the backbone of the class and the `.fit()` method for you here below. \n",
    "\n",
    "NB: `data` won't have the \"price\" column as it is passed as `labels`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30356032-eee4-454f-93c6-108c616e141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, data, labels=None) -> \"FeatureEngineeringTransformer\":\n",
    "        self.zipcode_groups = make_zipcode_groups(pd.concat([data[\"zipcode\"], labels], axis=1))\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, labels=None) -> pd.DataFrame:\n",
    "        pass # replace this line with your implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011efd6b-1e7f-46f0-90bf-e8f96a416373",
   "metadata": {},
   "source": [
    "If your implementation is correct the cell below will run successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830c337-0016-4202-9c54-1183fae7295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_trf = FeatureEngineeringTransformer()\n",
    "fe_trf.fit(train_set.drop(\n",
    "    columns=[\"price\"]),      # training set data\n",
    "    train_set[\"price\"]       # training set labels\n",
    ")\n",
    "fe_trf.transform(train_set.drop(columns=[\"price\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2fd73-622a-4f80-8b9d-0bf2c10d737b",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "One of the most important transformations you need to apply to your data is feature scaling. In the great majority of case, Machine Learning algorithms will not perform well when the input numerical attributes have very different scales.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale:\n",
    "* min-max scaling:  rescaling the range of features to scale the range in [0, 1] or [−1, 1] (using scikit-learn `MinMaxScaler`)\n",
    "* standardization: scales the data to have zero mean and variance = 1 (using scikit-learn `StandardScaler`).\n",
    "\n",
    "We will see an example of feature scaling below, when we'll show how all the preprocessing can be performed together building a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5a67f-0f4c-40f3-9322-43fecef43f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example courtesy of ChatGPT \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ages = [[30], [40], [50], [60]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_ages = scaler.fit_transform(ages)\n",
    "\n",
    "print(\"Original ages:\")\n",
    "print(ages)\n",
    "print(\"\\nScaled ages (min-max scaled):\")\n",
    "print(scaled_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example courtesy of ChatGPT \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ages = [[30], [40], [50], [60]]\n",
    "scaler = StandardScaler()\n",
    "standardized_ages = scaler.fit_transform(ages)\n",
    "\n",
    "print(\"Original ages:\")\n",
    "print(ages)\n",
    "print(\"\\nStandardized ages:\")\n",
    "print(standardized_ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9576875-68e4-4e9a-aa0a-a95101d9750f",
   "metadata": {},
   "source": [
    "## 5. Transformation Pipelines\n",
    "\n",
    "Pipeline object are useful to chain transformations (and potentially estimators) together, ensuring clean code and reproducibility.\n",
    "\n",
    "We are goin to use a `StandardScaler` directly after the `SimpleImputer` only for non-binary numerical features using a scikit-learn tranformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e4aa9-992c-40cb-9b8f-f205e5be54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cat_feats = [\"zipcode_group\"]\n",
    "binary_feats = [\"waterfront\", \"north_loc\", \"rural_east\", \"basement\", \"renovated\"] \n",
    "num_feats = [\n",
    "    el for el in list(\n",
    "        train_data.select_dtypes(include=[np.number])\n",
    "    ) if el not in binary_feats\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0a8a8-f02a-4177-98e3-78f12e4f2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "train_data_num_scaled = num_pipeline.fit_transform(train_data[num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce7c2d-2b8e-402b-a110-3f938df84c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b50af5-03e0-4eea-af47-edf9c1eff9d4",
   "metadata": {},
   "source": [
    "As you can see the data has all been scaled to be zero-centred and with variance = 1.\n",
    "\n",
    "Scikit-learn's built-in transformers output NumPy arrays by default. If we want to get a pandas data frame out, we can use the `set_output` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458a0e3-5911-4a58-83d2-a3362f111cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\").set_output(transform=\"pandas\")),\n",
    "    ('std_scaler', StandardScaler().set_output(transform=\"pandas\"))\n",
    "])\n",
    "train_data_num_scaled = num_pipeline.fit_transform(train_data[num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e4d16-af93-4cf2-94e1-d7cd248e8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a2120-1eb6-4a7f-9ab7-f5b7dc383bae",
   "metadata": {},
   "source": [
    "### 6. Chaining all together - `ColumnTransformer` and `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fd3aa-09f8-42eb-a474-f01e3af7680e",
   "metadata": {},
   "source": [
    "So far we have transformed the data step by step, and given different transformations to different features in our dataset. Now, ideally, we would like to chain all the data preparation steps in a single operation. This would simplify applying the tranformations to different data sets (i.e. training and test set) as well as ensuring the reproducibility of our data manipulation pipeline (all the transformations would be executed atomically).\n",
    "\n",
    "Until now, we have handled the categorical, binary and numerical columns separately. It would be more convenient if we had just one transformer capable to handle all columns, applying the appropriate transformations to each column. Solution: we can use scikit-learn ColumnTransformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99f43e-4be8-4287-b9d2-a694d9ab7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    (\n",
    "        (\"numerical\", num_pipeline, num_feats),\n",
    "        (\"categorical\", OneHotEncoder(categories='auto', sparse_output=False).set_output(transform=\"pandas\"), cat_feats),\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c66ad-0636-48f2-914f-0871655d9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_prepared = column_transformer.fit_transform(train_data)\n",
    "train_data_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6b1fa-f872-4d0a-be55-b955016abdc7",
   "metadata": {},
   "source": [
    "Most of the steps in the pipeline are now chained together. We are just missing the `engineering_features()` transformation step. We can use [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer) to convert `engineering_features()` to a scikit-learn's transformer.\n",
    "\n",
    "### **Exercise:** create a new pipeline chaining the `FeatureEngineeringTransformer` with the `column_transformer` we have defined above and apply the transformation to the original \"raw\" `train_set` dataset.\n",
    "\n",
    "Hint: you can also use the `make_pipeline()` utility function to easily create a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22eb74e-a2c0-4a72-a8b9-0c1459ef390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here:\n",
    "full_pipeline = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cffb4-eebe-4fac-b299-00d4c16297d9",
   "metadata": {},
   "source": [
    "If your implementation is correct the cells below will work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be0126-fb9d-410f-80e0-ba0301fa5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.fit(train_set.drop(columns=[\"price\"]), train_set[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b7c8b-3470-4e49-982e-bb69dde3ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_prepared =  full_pipeline.transform(train_set.drop(columns=[\"price\"]))\n",
    "train_data_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a3aa5-a313-4d2d-bef5-0378224d1c03",
   "metadata": {},
   "source": [
    "Great job: we now have all our data preparation pipeline encapsuled in the `full_pipeline` object.\n",
    "\n",
    "Now we can prepare the test set for the final evaluation as well.\n",
    "\n",
    "*Nota bene:* you must never fit an estimator (predictor or transformer) to the test set. This would mean leaking your test data into the training phase and would invalidate any conclusions on generalisation you may draw from evaluation on the test set. Hence, we must only use the `full_pipeline.transform()` method with our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaedbb8-7109-400b-8f07-e8052755f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prepared = full_pipeline.transform(test_set.drop(columns=[\"price\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fdcb0-6daa-40f6-9e66-dffaff240a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a52c6-512d-41c2-aed7-996b1dcdd94d",
   "metadata": {},
   "source": [
    "### 7. Save all the prepared data - train and test\n",
    "\n",
    "Let's add back the \"price\" column to our prepared datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d88f2-9cd2-43c5-95a4-c7a17fa823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_prepared = pd.concat([train_data_prepared, train_set[\"price\"]], axis=1)\n",
    "train_set_prepared.sample(5, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c2467-82d3-42ba-8ee7-d526579c3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_prepared = pd.concat([test_data_prepared, test_set[\"price\"]], axis=1)\n",
    "test_set_prepared.sample(5, random_state=77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394e977-9a95-4280-83f9-607289d29658",
   "metadata": {},
   "source": [
    "Let's save the datasets as CSV files in a newly created `prepared` subdirectory of `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6793c2b-f8d6-42cf-9870-09d55229294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../datasets/prepared/\", exist_ok=True)\n",
    "train_set_prepared.to_csv(\"../datasets/prepared/kd-housing-train.csv\", index=False)\n",
    "test_set_prepared.to_csv(\"../datasets/prepared/kd-housing-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_prepared.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da5398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
